{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfed58a9",
   "metadata": {},
   "source": [
    "# Computational Graph\n",
    "\n",
    "A computational graph is a network of nodes where each node performs a computation and stores a variable. Edges connect nodes, representing relationships between variables. The graph can be directed or undirected. In directed graphs, nodes compute their variables based on incoming edge-connected nodes. Undirected graphs allow for bidirectional relationships. Most practical computational graphs, like neural networks, are directed acyclic graphs. Parameters associated with edges in machine learning problems are learned from data, reflecting relationships between attribute values. Input nodes are fixed to data attributes, while others compute values using node-specific functions. Some computed node values are compared to observed target values, and edge parameters are adjusted to align computed and observed values, allowing the learning of a function connecting input and target attributes in the data.\n",
    "\n",
    "This notebook focusses on directed acyclic graphs with continuous, deterministic variables. A _feed-forward neural network_ is an important special case of this type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc8d6",
   "metadata": {},
   "source": [
    "## Directed acyclic computational graphs\n",
    "\n",
    "In such a graph, each node is associated with a variable. A set of directed edges connect nodes, which shows how nodes are related. Edges might be associated with learnable parameters. A variable in a node is either fixed externally (for input nodes with no incoming edges), or it is computed as a function of the variables in the tail ends of edges incoming into the node and the learnable parameters on the incoming edges.\n",
    "\n",
    "__Types of Nodes__\n",
    "A computational graph has three types of nodes: input, output, and hidden nodes. Input nodes take external inputs, output nodes give final results, and hidden nodes hold in-between values. Each hidden and output node evaluates a simple (_local_) function based on its input. The combined calculations throughout the graph create the _global_ function from input to output. The node-specific functions also use parameters associated with their incoming edges, and the inputs along those edges are scaled with the weights. By choosing the right weights, we can control the overall function of the graph. This function is often learned by giving the graph input-output examples (training data) and adjusting the weights to make the predicted outputs match the real ones.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae88ce79",
   "metadata": {},
   "source": [
    "__Example 1__   Let $ f: \\mathbb{R}^3 \\to \\mathbb{R}$ with $f(x_1, x_2, x_3) = \\ln(x_1x_2)\\ \\exp(x_1x_2x_3)\\ \\sqrt{x_2 + 7x_3}$. \n",
    "\n",
    "a. Construct a computational graph that represents global function $f$.\n",
    "\n",
    "b. How many _(hidden) layers_ your computational graph has?\n",
    "\n",
    "c. Compute the output if $(x_1, x_2, x_3) = (2,2,1)$.\n",
    "\n",
    "d. Readjust the wightes (coefficients) to have an output closer to 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde2f3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "877a0da8",
   "metadata": {},
   "source": [
    "__Example 2__ Design a computational graph that can perform linear regression.\n",
    "\n",
    "Hint: A linear regression can be stated using a matrix equation: $ Ax = b$ ; so you need a _single-layer_ computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92265c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c3b8b4a",
   "metadata": {},
   "source": [
    "__Example 3__ Design a computational graph that can perform SVD.\n",
    "\n",
    "Hint: Think of SVD as a linear transformation. What does it do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94845ba7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db2169f9",
   "metadata": {},
   "source": [
    "## Neural Networks as Directed Computational Graphs\n",
    "\n",
    "Neural networks represent the most common use case of a multi-layer computational graph. The nodes are (typically) arranged in layerwise fashion, so that all nodes in layer-i are connected to nodes in layer-(i + 1) (and no other layer). The vector of variables in each layer can be written as a vector-to-vector function of the variables in the previous layer.\n",
    "\n",
    "\n",
    "For example, Let's start with $M$ different images (the training set). An image is a set of $p$ small pixels - or a vector $v = (v_1, \\ldots, v_p)$. The component $v_i$ tells us the \"grayscale\" of the $i$th pixel in the image: how dark or light it is. So we have $M$ images (vectors), each with $p$ features ($p$-dimensional). Let's assume there is a function $w = F (v)$ that can take an image and tell us which number it is (like turning a picture of a number into the actual number). Our goal is to find a rule that works well for the images we already have (in $M$) and also for new images (test set). To that end, we think of such function as a global function of a computational graph. Let's explore our options!\n",
    "\n",
    "1- Assuming $F$ is a linear map, what is the domain and range of of $F$?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "\n",
    "2- Design a computational graph for such $F$.\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "\n",
    "We started with a linear map for simplicity. It turns out the _linearity_ is far too limited. For example, artistically, two $0$'s could make an $8$. $1$ and $0$ could combine into a hand-written $9$ or possibly a $6$. Images don't add. \n",
    "\n",
    "_What would be the best class of functions?_  There is no claim that the absolute best class of functions exists, but how about to choose a class that are very _close_ to linear functions? So we dont lose simplicity but we increase our options (things that linear model can't encode). The answer is the class of [continuous piecewise linear](https://en.wikipedia.org/wiki/Piecewise_linear_function) (CPL) functions $F: \\mathbb{R}^p \\to \\mathbb{R}^{10}$!(think of [Triangle_mesh](https://en.wikipedia.org/wiki/Triangle_mesh)), for example!) So in that case, our nodes are the intersections of of lines, planes or hyperplanes. \n",
    "\n",
    "\n",
    "Let's construction of a piecewise linear function of the data vector $v$. We start with your design in part two. Lets call that the first step or the first function: $F_1(v) = A_1v$. Then we add a vector $b$ to it to make it an offine function: $F_2(v)= F_1(v) + b_1$. Next step is to set all negative components of $F_2(v)$ to $0$ (this is the nonlinear step) and denote the resulting vector by $F_2(v)+$. Next, we multiply $F_2(v)+$ by another matrix $A_2$ to produce $w$: $ F(v) = A_2(F_2(v))+$, or $w = A_2(A_1v+b_1)+$. \n",
    "\n",
    "3- Design a computational graph for this new $F$. \n",
    "\n",
    "__Answer:__\n",
    "\n",
    " \n",
    "The optimization problem in deep learning is to compute weights $A_i$ and $b_i$ that make the outputs $F(v)$ as close to $w(v)$ (the digit that the image $v$ represents) as possible. Next week we will discuss how to minimize $\\|F(v) - w(v)\\|$ using the backpropagation algorithm.\n",
    "\n",
    "\n",
    "\n",
    "**Remark:** The nonlinear function introduced in the above setting is called the _rectified linear unit (ReLU)_: $ReLU(x) = \\max(x, 0)$. It's evident that it doesn't have a continuous derivative. Therefore, it's reasonable to consider an alternative function with continuous derivatives to aid in the optimization problem. For this reason, ReLU was initially smoothed into a logistic curve like $\\frac{1}{1 + e^{-x}}$. However, this approach turned out to be incorrect. In fact, during the historic competition in 2012 that aimed to identify 1.2 million images in ImageNet, the groundbreaking neural network in AlexNet had 60 million weights distributed across eight layers. Its accuracy, achieved after five days of stochastic gradient descent, cut the error rate in deep learning in half. Not only is continuous piecewise linear (CPL) a powerful class of approximators, but it is also convenient as it is close under addition, maximization, and composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181383e",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d427ae",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. Gilbert Strang, \"Linear Algebra and Learning from Data,\" 2019. [Online] Available at: math.mit.edu/learningfromdata.\n",
    "\n",
    "2. Charu C. Aggarwal, \"Linear Algebra and Optimization for Machine Learning,\" 2019.\n",
    "\n",
    "3. M. P. Deisenroth, A. A. Faisal, C. S. Ong, \"Mathematics for Machine Learning,\" 2020. [Online] Available at: https://mml-book.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b3bed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "729e4673",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa106486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
